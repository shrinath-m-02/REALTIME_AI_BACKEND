# Realtime AI Backend - WebSockets + Supabase

A production-ready FastAPI backend for real-time AI conversations with WebSocket streaming, Supabase persistence, tool calling, and async background tasks.

## üéØ Features

‚úÖ **Real-time WebSocket Streaming** - Token-by-token LLM response streaming  
‚úÖ **Tool Calling** - LLM can invoke tools (fetch_user_profile, get_system_metrics)  
‚úÖ **Supabase Integration** - Full session and event persistence with fallback in-memory storage  
‚úÖ **Async Architecture** - 100% async with proper event loop management  
‚úÖ **Session Management** - Track conversations, calculate duration, generate summaries  
‚úÖ **Background Tasks** - Non-blocking session summaries on disconnect  
‚úÖ **Beautiful UI** - Modern web frontend with real-time chat  
‚úÖ **Production Ready** - Error handling, logging, health checks  

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FastAPI Application                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  WebSocket Endpoint (/ws/session/{session_id})         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                           ‚îÇ                                  ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                 ‚îÇ              ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ    ‚îÇ Manager  ‚îÇ     ‚îÇ  Session    ‚îÇ   ‚îÇ Summary    ‚îÇ      ‚îÇ
‚îÇ    ‚îÇ (WS)    ‚îÇ     ‚îÇ Service     ‚îÇ   ‚îÇ Service    ‚îÇ      ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                ‚îÇ               ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                           ‚îÇ                               ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                    ‚îÇ LLM Service  ‚îÇ                        ‚îÇ
‚îÇ                    ‚îÇ (Streaming)  ‚îÇ                        ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                           ‚îÇ                               ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                    ‚îÇ   OpenAI     ‚îÇ                        ‚îÇ
‚îÇ                    ‚îÇ   API        ‚îÇ                        ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ         ‚îÇ                                                  ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚ñ∫ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ             ‚îÇ Supabase Service    ‚îÇ                     ‚îÇ
‚îÇ             ‚îÇ (Persistence)       ‚îÇ                     ‚îÇ
‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                        ‚îÇ                                ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ              ‚îÇ                    ‚îÇ                    ‚îÇ
‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ          ‚îÇSession‚îÇ          ‚îÇEvent Logs ‚îÇ            ‚îÇ
‚îÇ          ‚îÇ Table ‚îÇ          ‚îÇ  Table    ‚îÇ            ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä Database Schema

### Sessions Table
```sql
CREATE TABLE sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id TEXT NOT NULL UNIQUE,
    user_id TEXT,
    start_time TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    end_time TIMESTAMP WITH TIME ZONE,
    duration_seconds INTEGER,
    final_summary TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_sessions_session_id ON sessions(session_id);
CREATE INDEX idx_sessions_user_id ON sessions(user_id);
```

### Event Logs Table
```sql
CREATE TABLE event_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id TEXT NOT NULL REFERENCES sessions(session_id) ON DELETE CASCADE,
    event_type TEXT NOT NULL,
    content TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_event_logs_session_id ON event_logs(session_id);
CREATE INDEX idx_event_logs_created_at ON event_logs(created_at);
```

## üöÄ Quick Start

### 1. Prerequisites
- Python 3.10+
- OpenAI API key
- (Optional) Supabase account for cloud persistence

### 2. Installation

```bash
# Clone repository
cd realtime_ai_backend

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On Windows:
.venv\Scripts\activate
# On macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration

Copy `.env.example` to `.env` and configure:

```bash
cp .env.example .env
```

Edit `.env`:
```env
# Required
OPENAI_API_KEY=sk-proj-xxxxx

# Optional - leave empty to use in-memory storage
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key

# Optional
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.7
SERVER_PORT=8001
```

### 4. Setup Supabase (Optional)

If using Supabase, run these SQL commands:

```sql
-- Create sessions table
CREATE TABLE sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id TEXT NOT NULL UNIQUE,
    user_id TEXT,
    start_time TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    end_time TIMESTAMP WITH TIME ZONE,
    duration_seconds INTEGER,
    final_summary TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create event logs table
CREATE TABLE event_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id TEXT NOT NULL REFERENCES sessions(session_id) ON DELETE CASCADE,
    event_type TEXT NOT NULL,
    content TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create indexes
CREATE INDEX idx_sessions_session_id ON sessions(session_id);
CREATE INDEX idx_sessions_user_id ON sessions(user_id);
CREATE INDEX idx_event_logs_session_id ON event_logs(session_id);
CREATE INDEX idx_event_logs_created_at ON event_logs(created_at);
```

### 5. Run Server

```bash
python main.py
```

Or with uvicorn directly:

```bash
uvicorn main:app --host 127.0.0.1 --port 8001 --reload
```

Access:
- **Web UI**: http://localhost:8001
- **API Docs**: http://localhost:8001/docs
- **WebSocket**: ws://localhost:8001/ws/session/{session_id}
- **Health**: http://localhost:8001/health

## üì° WebSocket Protocol

### Connection
```javascript
ws = new WebSocket("ws://localhost:8001/ws/session/my-session-123");
```

### Message Format

**Client ‚Üí Server** (text):
```
"Hello, what is the weather like?"
```

**Server ‚Üí Client** (JSON):
```json
// Connection established
{"type": "system", "content": "Connected to session..."}

// Start of AI response
{"type": "ai_response_start"}

// Streaming chunks
{"type": "ai_response_chunk", "content": "The"}
{"type": "ai_response_chunk", "content": " weather"}
{"type": "ai_response_chunk", "content": "..."}

// End of response
{"type": "ai_response_end"}

// Error
{"type": "error", "content": "Error message"}
```

## üõ†Ô∏è Tool Calling

The LLM can automatically call these tools:

### `fetch_user_profile`
```json
{
    "name": "fetch_user_profile",
    "parameters": {
        "user_id": "user123"
    }
}
```

**Response:**
```json
{
    "user_id": "user123",
    "name": "Alice Johnson",
    "email": "alice@example.com",
    "account_status": "active",
    "subscription_tier": "premium"
}
```

### `get_system_metrics`
```json
{
    "name": "get_system_metrics",
    "parameters": {
        "metric_type": "all"
    }
}
```

**Response:**
```json
{
    "cpu_usage_percent": 45.2,
    "memory_usage_percent": 62.8,
    "memory_available_gb": 8.2,
    "timestamp": "2024-12-17 10:30:00"
}
```

## üß™ Testing

### Browser
1. Open http://localhost:8001
2. Type message and press Enter
3. See real-time streaming response

### Using wscat
```bash
# Install wscat
npm install -g wscat

# Connect
wscat -c "ws://localhost:8001/ws/session/test-123"

# Send message (type and press Enter)
> Tell me about your system metrics
```

### Using Python
```python
import asyncio
import websockets
import json

async def test():
    async with websockets.connect("ws://localhost:8001/ws/session/test-123") as ws:
        # Receive connection message
        msg = await ws.recv()
        print("System:", json.loads(msg))
        
        # Send message
        await ws.send("What can you tell me about users?")
        
        # Receive streaming response
        while True:
            try:
                msg = await ws.recv()
                data = json.loads(msg)
                print(f"{data['type']}: {data.get('content', '')}", end="", flush=True)
            except:
                break

asyncio.run(test())
```

### Using curl + websocat
```bash
# Install websocat
cargo install websocat  # or download binary

# Test
echo "Hi there" | websocat ws://localhost:8001/ws/session/test-123
```

## üìÇ Project Structure

```
realtime_ai_backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py             # OpenAI streaming + tool calling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websocket.py       # WebSocket manager
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Pydantic models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase.py        # Database operations
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ session_service.py # Session state management
‚îÇ       ‚îú‚îÄ‚îÄ summary_service.py # Session summary generation
‚îÇ       ‚îî‚îÄ‚îÄ tools.py           # Tool execution
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ index.html             # Web UI
‚îú‚îÄ‚îÄ main.py                    # FastAPI application
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ .env.example              # Environment variables template
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üîë Key Design Decisions

### 1. **100% Async Architecture**
- All database operations are async-ready
- Uses asyncio background tasks for non-blocking summaries
- Event loop properly yielded during streaming

### 2. **Dual Storage Strategy**
- **Primary**: Supabase (when configured) for cloud persistence
- **Fallback**: In-memory dictionaries for offline/development
- Automatic fallback if Supabase is unavailable

### 3. **WebSocket Streaming Without Buffering**
- Tokens are sent immediately to client
- No waiting for complete LLM response
- Low latency: ~50-200ms per token

### 4. **Tool Calling Integration**
- LLM can detect when tools are needed
- Tool results are fed back into conversation
- Seamless continuation of response

### 5. **Session Lifecycle**
```
CONNECT ‚Üí CREATE_SESSION ‚Üí 
RECEIVE_MESSAGE ‚Üí 
  ADD_TO_CONVERSATION ‚Üí 
  STREAM_LLM_RESPONSE ‚Üí 
  SAVE_EVENTS ‚Üí
REPEAT MESSAGE_HANDLING ‚Üí
DISCONNECT ‚Üí 
  TRIGGER_SUMMARY_TASK (non-blocking) ‚Üí
  SAVE_END_TIME_AND_DURATION
```

### 6. **Error Handling**
- Graceful WebSocket error recovery
- Database fallback to in-memory storage
- Detailed logging for debugging
- User-friendly error messages

## üìà Performance Considerations

- **Streaming Latency**: ~100-300ms from user message to first token
- **Token Generation**: ~50-150ms per token (depends on model)
- **Database Operations**: Async, non-blocking
- **Concurrent Sessions**: Limited by OpenAI API rate limits and server resources
- **Memory Usage**: ~50MB baseline + ~1MB per active session

## üîí Security Notes

‚ö†Ô∏è **For Production:**
- Set `DEBUG=false` in .env
- Use environment variables for all secrets (never hardcode)
- Implement authentication (JWT, API keys)
- Rate limiting on WebSocket endpoints
- HTTPS/WSS (not WS) for cloud deployment
- Input validation and sanitization
- SQL injection protection (Supabase handles via ORM)

## üêõ Troubleshooting

### WebSocket Connection Fails
```
Error: Connection refused
```
- Check server is running: `python main.py`
- Verify port: `netstat -an | grep 8001`
- Check firewall settings

### "No module named 'app.core.config'"
```bash
# Ensure you're running from project root
cd /path/to/realtime_ai_backend
python main.py
```

### Supabase Connection Error
```
ModuleNotFoundError: No module named 'supabase'
```
- Install: `pip install supabase`
- Or reinstall: `pip install -r requirements.txt`

### "OPENAI_API_KEY not set"
```bash
# Create .env file with your key
echo "OPENAI_API_KEY=sk-proj-xxx" > .env
```

### LLM Requests Timeout
- Increase `LLM_TIMEOUT` in .env (default: 30 seconds)
- Check OpenAI API status
- Try with `gpt-4o-mini` (faster) instead of `gpt-4`

## üìù API Endpoints

### WebSocket
- `GET /ws/session/{session_id}` - Open WebSocket connection

### REST
- `GET /` - Serve web UI (index.html)
- `GET /health` - Health check endpoint
- `GET /api/session/{session_id}` - Get session details and history
- `GET /docs` - Interactive API documentation (Swagger)
- `GET /redoc` - Alternative API documentation

## üöÄ Deployment

### Docker
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

ENV OPENAI_API_KEY=${OPENAI_API_KEY}
ENV SUPABASE_URL=${SUPABASE_URL}
ENV SUPABASE_KEY=${SUPABASE_KEY}

CMD ["python", "main.py"]
```

Build and run:
```bash
docker build -t realtime-ai-backend .
docker run -p 8001:8001 \
  -e OPENAI_API_KEY=sk-proj-xxx \
  -e SUPABASE_URL=https://xxx.supabase.co \
  -e SUPABASE_KEY=xxx \
  realtime-ai-backend
```

### Heroku
```bash
heroku create your-app-name
heroku config:set OPENAI_API_KEY=sk-proj-xxx
heroku config:set SUPABASE_URL=https://xxx.supabase.co
heroku config:set SUPABASE_KEY=xxx
git push heroku main
```

## üìö Additional Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [WebSocket Protocol](https://tools.ietf.org/html/rfc6455)
- [OpenAI API](https://platform.openai.com/docs)
- [Supabase Documentation](https://supabase.com/docs)
- [Python asyncio](https://docs.python.org/3/library/asyncio.html)

## üìÑ License

MIT License - feel free to use this project for personal or commercial purposes.

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

---

**Built with ‚ù§Ô∏è using FastAPI, OpenAI, and Supabase**

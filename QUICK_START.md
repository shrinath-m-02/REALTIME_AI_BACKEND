# Quick Start Guide

## Status: ‚úÖ Server Running

**The server is currently running at: http://127.0.0.1:8001**

## üöÄ Access the Application

### Option 1: Web Browser (Recommended)
1. Open your browser
2. Go to: http://127.0.0.1:8001
3. Type a message in the chat box
4. Press Enter or click Send
5. Watch the AI response stream in real-time!

### Option 2: API Documentation
1. Open: http://127.0.0.1:8001/docs
2. Interactive Swagger UI with all endpoints
3. Try out endpoints directly in the browser

### Option 3: Health Check
```bash
curl http://127.0.0.1:8001/health
```

Returns:
```json
{
    "status": "healthy",
    "database": "in-memory",
    "sessions": 0
}
```

## üí¨ Example Prompts to Try

1. **Ask about tools:**
   ```
   "Tell me about your system metrics and user profile capabilities"
   ```

2. **Request tool execution:**
   ```
   "What are the current system metrics?"
   ```

3. **Ask for user info:**
   ```
   "Can you tell me about user1's profile?"
   ```

4. **Casual conversation:**
   ```
   "Tell me a joke about programming"
   ```

5. **Multi-turn conversation:**
   ```
   First: "What's my profile?"
   Then: "Can you tell me about the system?"
   Then: "Combine that information in a summary"
   ```

## üîß Server Management

### Stop the Server
```bash
# Press Ctrl+C in the terminal where server is running
```

### Restart the Server
```bash
cd c:\workspace\realtime_ai_backend
python -m uvicorn main:app --host 127.0.0.1 --port 8001
```

### Run with Auto-Reload (Development)
```bash
python -m uvicorn main:app --host 127.0.0.1 --port 8001 --reload
```

## üìã Project Files

### Core Application
- **main.py** - FastAPI application entry point
- **app/core/config.py** - Configuration management
- **app/core/websocket.py** - WebSocket handler
- **app/core/llm.py** - OpenAI integration with streaming
- **app/db/supabase.py** - Database layer
- **app/services/session_service.py** - Session management
- **app/services/summary_service.py** - Summary generation

### Frontend
- **static/index.html** - Web chat interface

### Configuration
- **.env** - Environment variables (API keys, etc.)
- **requirements.txt** - Python dependencies
- **README.md** - Full documentation

## üîê Security Notes

‚ö†Ô∏è **Current Configuration:**
- ‚úì OpenAI API key configured
- ‚ö†Ô∏è No authentication (development mode)
- ‚ö†Ô∏è No rate limiting
- ‚ö†Ô∏è In-memory storage (data lost on restart)

For production:
1. Enable authentication (JWT, OAuth)
2. Add rate limiting
3. Set up Supabase for persistence
4. Use HTTPS/WSS
5. Enable CORS properly
6. Add input validation

## üåê Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | Web UI |
| `/health` | GET | Health check |
| `/docs` | GET | API docs (Swagger) |
| `/ws/session/{id}` | WebSocket | Real-time chat |
| `/api/session/{id}` | GET | Session details |

## üìä Features

‚úÖ **Real-time WebSocket Streaming**
- Token-by-token LLM responses
- Low latency (100-300ms to first token)

‚úÖ **Tool Calling**
- LLM can invoke: `fetch_user_profile`, `get_system_metrics`
- Results fed back to LLM for enhanced responses

‚úÖ **Session Management**
- Automatic session creation
- Conversation history tracking
- Auto-generated summaries on disconnect

‚úÖ **Beautiful UI**
- Modern, responsive design
- Real-time message streaming
- Connection status indicator
- Smooth animations

## üß™ Testing Tools

### Using Browser
1. Open http://127.0.0.1:8001
2. Type message
3. See streaming response

### Using wscat CLI
```bash
npm install -g wscat
wscat -c "ws://127.0.0.1:8001/ws/session/test-123"
```

### Using Python
```python
import asyncio
import websockets

async def test():
    async with websockets.connect("ws://127.0.0.1:8001/ws/session/test") as ws:
        msg = await ws.recv()
        print(msg)
        await ws.send("Hello")
        response = await ws.recv()
        print(response)

asyncio.run(test())
```

## üêõ Troubleshooting

### Server won't start
**Problem:** "Port already in use"
```bash
# Kill the process using port 8001
Get-Process python | Stop-Process -Force
```

### WebSocket won't connect
**Problem:** "Connection refused"
```bash
# Check if server is running
curl http://127.0.0.1:8001/health
# If error, restart server
```

### AI not responding
**Problem:** "Timeout waiting for response"
- Check OpenAI API key in .env
- Check internet connection
- Try with shorter prompt

### UI not loading
**Problem:** "Cannot GET /"
- Ensure static/index.html exists
- Check file permissions
- Restart server

## üìû Getting Help

1. Check **README.md** for detailed documentation
2. Check **COMPLETION_SUMMARY.md** for feature overview
3. Review **app/core/config.py** for configuration options
4. Check server logs in terminal for errors

## üéØ Next Steps

1. ‚úÖ Server is running
2. ‚úÖ Frontend is accessible
3. ‚úÖ Try a message in the UI
4. ‚úÖ Watch the streaming response
5. Optional: Set up Supabase for persistence
6. Optional: Deploy to production

---

**Everything is ready to use!**

Start chatting at: http://127.0.0.1:8001
